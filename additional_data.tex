\label{sec:supplement}

Figure \ref{fig:MPIwithIO-Bridges} shows performance of the RMSD task on PSC Bridges. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-t_total-Bridges.pdf}
  \caption{Scaling total (five repeats)}
  \label{fig:MPIscaling-Bridges}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-speed_up-Bridges.pdf}
  \caption{Speed-up (five repeats)}
  \label{fig:MPIspeedup-Bridges}
\end{subfigure}
\bigskip

\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-time_comp_IO_comparison-Bridges.pdf}
  \captionsetup{format=hang}
\caption{Scaling for different components (five repeats)}
\label{fig:ScalingComputeIO-Bridges}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-BarPlot-rank-comparison_72_4-Bridges.pdf}
  \captionsetup{format=hang}
  \caption{Time comparison on different parts of the calculations per MPI rank (example)}
  \label{fig:MPIranks-Bridges}
\end{subfigure}

\caption{Performance of the RMSD task with MPI which is I/O-bound $\RcompIO \approx 0.3$ on PSC Bridges.
Results are communicated back to rank 0 (communications included). Five independent repeats were performed to collect statistics. (a-c) The error bars show
standard deviation with respect to mean. (d) Compute \tcomp, IO \tIO, communication \tcomm, ending the for loop \text{$t_{\text{end\_loop}}$},
  opening the trajectory \text{$t_{\text{opening\_trajectory}}$}, and overheads \text{$t_{\text{overhead1}}$}, \text{$t_{\text{overhead2}}$} per MPI rank (See Table \ref{tab:notation} for the definition).
These are data from one run of the five repeats. MPI ranks 0 and 70 are stragglers. \textbf{Note:} In serial, there is no communication.}
\label{fig:MPIwithIO-Bridges}
\end{figure} 

%%%%%%%%%%%%%%%%%%

Figure \ref{fig:MPIwithIO-SuperMIC} shows performance of the RMSD task on SuperMIC. 

\begin{figure}[ht!]
\centering
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-t_total-SuperMIC.pdf}
  \caption{Scaling total (five repeats)}
  \label{fig:MPIscaling-SuperMIC}
\end{subfigure}
\hfill
\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-speed_up-SuperMIC.pdf}
  \caption{Speed-up (five repeats)}
  \label{fig:MPIspeedup-SuperMIC}
\end{subfigure}
\bigskip

\begin{subfigure}{.4\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-time_comp_IO_comparison-SuperMIC.pdf}
  \captionsetup{format=hang}
\caption{Scaling for different components (five repeats)}
\label{fig:ScalingComputeIO-SuperMIC}
\end{subfigure}
\hfill
\begin{subfigure} {.5\textwidth}
  \includegraphics[width=\linewidth]{figures/main-RMSD-BarPlot-rank-comparison_80_5-SuperMIC.pdf}
  \captionsetup{format=hang}
  \caption{Time comparison on different parts of the calculations per MPI rank (example)}
  \label{fig:MPIranks-SuperMIC}
\end{subfigure}

\caption{Performance of the RMSD task with MPI which is I/O-bound $\RcompIO \approx 0.3$ on SuperMIC.
Results are communicated back to rank 0 (communications included). Five independent repeats were performed to collect statistics. (a-c) The error bars show
standard deviation with respect to mean. (d) Compute \tcomp, IO \tIO, communication \tcomm, ending the for loop \text{$t_{\text{end\_loop}}$},
  opening the trajectory \text{$t_{\text{opening\_trajectory}}$}, and overheads \text{$t_{\text{overhead1}}$}, \text{$t_{\text{overhead2}}$} per MPI rank (See Table \ref{tab:notation} for the definition).
These are data from one run of the five repeats. MPI ranks 0 and 70 are stragglers. \textbf{Note:} In serial, there is no communication.}
\label{fig:MPIwithIO-SuperMIC}
\end{figure} 

%%%%%%%%%%%%%%%%%
Figure \ref{fig:comparison_efficiency_clusters} shows comparison of the parallel efficiency of the RMSD task between different test cases on SDSC Comet, PSC Bridges, and LSU SuperMIC.  

 \begin{figure}[ht!]
\centering
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Efficiency_all_Comet.pdf}
  \caption{Comet}
  \label{fig:comparison_efficiency}
\end{subfigure}
\hfill
\begin{subfigure}{.35\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Efficiency_all_Bridges.pdf}
  \caption{Bridges}
  \label{fig:comparison_efficiency_Bridges}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Efficiency_all_SuperMIC.pdf}
  \caption{SuperMIC}
  \label{fig:comparison_efficiency_SuperMIC}
\end{subfigure}

\caption{Comparison of the parallel efficiency between different test cases on (a) SDSC Comet, (b) Bridges, and (c) SuperMIC.
Five repeats were performed to collect statistics and error bars show standard deviation with respect to mean.}
\label{fig:comparison_efficiency_clusters}
\end{figure} 

Figure \ref{fig:MPIwithIO-split-SuperMIC} shows how RMSD task scales with the increase in the number of cores when the trajectories are split using GA and without GA on SuperMIC.  
 
 \begin{figure}[ht!]
\centering
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_IO_compute_scaling_traj_splitting-SuperMIC.pdf}
  \captionsetup{format=hang}
  \caption{Scaling for different components}
  \label{fig:MPIscaling-SuperMIC}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_tot_time_traj_splitting-SuperMIC.pdf}
  \caption{Scaling total}
  \label{fig:MPItottime-SuperMIC}
\end{subfigure}
\hfill
\begin{subfigure}{.3\textwidth}
  \includegraphics[width=\linewidth]{figures/Comparison_Speed_UP_traj_splitting-SuperMIC.pdf}
  \caption{Speed-up}
  \label{fig:MPIspeedup-SuperMIC}
\end{subfigure}

\caption{Comparison on the performance of the RMSD task with MPI when the trajectories are split using global array and without global array ($\RcompIO \approx 0.3$) on SuperMIC.
In case of global array, all ranks update the global array (\text{$ga_{\text{put}}$}) and rank 0 accesses the whole RMSD array through the global memory address (\text{$ga_{\text{get}}$}).
Five repeats were performed to collect statistics. (a) Compute and I/O scaling versus number of processes (b) Total time scaling versus number of processes (c) Speed-up (a-c) The error bars show standard deviation with respect to mean.}
\label{fig:MPIwithIO-split-SuperMIC}
\end{figure}

