\label{sec:background}

In our previous work we found that straightforward implementation of simple parallelization with a split-apply-combine algorithm in Python failed to scale beyond a single compute node~\cite{Khoshlessan:2017ab} because a few tasks (MPI-ranks or Dask~\citep{Rocklin:2015aa} processes) took much longer than the typical task and so limited the overall performance.
However, the cause for these \emph{straggler} tasks remained obscure.
Here, we studied the straggler problem in the context of an MPI-parallelized trajectory analysis algorithm in Python and investigated solutions to overcome it.
We briefly review stragglers in section~\ref{sec:stragglers} and summarize existing approaches to parallel trajectory analysis in section~\ref{sec:otherparallel}.

\subsection{Stragglers}
\label{sec:stragglers}


\emph{Stragglers} or \emph{outliers} were traditionally considered in the context of MapReduce jobs that consist of multiple tasks that all have to finish for the job to succeed: A straggler was a task that took an ``unusually long time time to complete'' \cite{Dean2004} and therefore substantially impeded job completion.
In general, any component of a parallel workflow whose runtime exceeds a typical run time (for example, 1.5 times the median runtime) can be considered a straggler \cite{Ananthanarayanan:2010aa}.
Stragglers are a challenge for improving performance on HPC resources \cite{Garraghan2016}; they are a known problem in frameworks such as MapReduce~\cite{Dean2004, Ananthanarayanan:2010aa}, Spark~\cite{Kyong2017,Ousterhout2017,Gittens2016}, Hadoop~\cite{Dean2004}, cloud data centers~\cite{Schmidt2016}, and have a high impact on performance and energy consumption of big data systems~\cite{Tien-2017}.
Both internal and external factors are known to contribute to stragglers. 
Internal factors include heterogeneous capacity of worker nodes and resource competition due to other tasks running on the same worker node.
External factors include resource competition due to co-hosted applications, input data skew, remote input or output source being too slow,  faulty hardware~\cite{Chen2014,Dean2004}, and node mis-configuration~\cite{Dean2004}.
Competition over scarce resources \cite{Ananthanarayanan:2010aa}, in particular the network bandwidth, was found to lead to stragglers in writing on Lustre file systems \cite{Xie:2012aa}.
Garbage collection~\cite{Kyong2017,Ousterhout2017}, Java virtual machine (JVM) positioning to cores~\cite{Kyong2017}, delays introduced while the tasks move from the scheduler to execution~\cite{Gittens2016}, disk I/O during shuffling, Java's just-in-time compilation~\cite{Ousterhout2017}, and output skew~\cite{Ousterhout2017} were also found to introduce stragglers.
In addition to these reasons, stragglers on Spark were attributed to the overall performance of workers or competition between resources~\cite{Yang2016}.
Garraghan et al.~\cite{Garraghan2016} reported high CPU utilization, disk utilization, unhandled I/O access requests, and network package loss as the most frequent reasons for stragglers on Virtualized Cloud Data-centers.
A wide variety of approaches have been investigated for detecting and mitigating stragglers, including tuning resource allocation and parallelism such as breaking the workload into many small tasks that are dynamically scheduled at runtime~\cite{Rosen2012}, slow Node-Threshold~\cite{Dean2004}, speculative execution~\cite{Dean2004} and cause/resource-aware task management \cite{Ananthanarayanan:2010aa}, sampling or data distribution estimation techniques, SkewTune to avoid data imbalance~\cite{Kwon2012}, dynamic work rebalancing~\cite{Schmidt2016}, blocked time analysis~\cite{Ousterhout2015}, and intelligent scheduling~\cite{AWE-WQ2014}. 

In the present study, we identified the cause of straggler tasks in the context of analyzing large MD trajectories and investigated solutions for improving performance.
Even though these solutions were specifically applied in the context of the \package{MDAnalysis} package, all these principles and  techniques are potentially applicable to data analysis in other Python-based libraries.

\subsection{Other Packages with Parallel Analysis Capabilities}
\label{sec:otherparallel}

Different approaches to parallelizing the analysis of MD trajectories have been proposed.
HiMach~\cite{himach-2008} introduces scalable and flexible parallel Python framework to deal with massive MD trajectories, by combining and extending Google's MapReduce and the VMD analysis tool~\cite{Hum96}. 
HiMach's runtime is responsible to parallelize and distribute Map and Reduce classes to assigned cores.
HiMach uses parallel I/O for file access during map tasks and \text{MPI\_Allgather} in the reduction process. 
HiMach, however, does not discuss parallel analysis of analysis types that cannot be implemented via MapReduce.
Furthermore, HiMach is not available under an open source license, which makes it difficult to integrate community contributions and add new state-of-the-art methods.

Wu et. al.~\cite{Wu_et.al} present a scalable parallel framework for distributed-memory post-simulation data analysis.
This work consists of an interface that allows a user to write analysis programs sequentially, and the machinery that ensures these programs execute in parallel automatically. 
The main components of the proposed framework are (1) domain decomposition that splits computational domain into blocks with specified boundary conditions, (2) HDF5 based parallel I/O (3) data exchange that communicates ghost atoms between neighbor blocks, and (4) parallel analysis implementation of a real-world analysis application.
This work does not discuss analysis methods which cannot be implemented using MapReduce and is limited to HDF5 file format.

Zazen~\cite{Zazen} is a novel task-assignment protocol to overcome the I/O bottleneck for many I/O bound tasks. This protocol caches a copy of simulation output files on the local disks of the compute nodes of a cluster, and uses co-located data access with computation. 
Zazen is implemented in a parallel disk cache system and avoids the overhead associated with querying metadata servers by reading data in parallel from local disks.
The approach has been used to improve the performance of HiMach~\cite{himach-2008}.
It, however, advocates a specific architecture where a parallel supercomputer, which runs the simulations, immediately pushes the trajectory data to a local analysis cluster where trajectory fragments are cached on node-local disks.
In the absence of such a specific  workflow, one would need to stage the trajectory across nodes, and the time for data distribution is likely to reduce any gains from the parallel analysis.

VMD~\cite{Hum96, VMD2013} provides molecular visualization and analysis tool through algorithmic and memory efficiency improvements, vectorization of key CPU algorithms, GPU analysis and visualization algorithms, and good parallel I/O performance on supercomputers. It is one of the most advanced programs for the visualization and analysis of MD simulations. It is, however, a large monolithic program, that can only be driven through its built-in Tcl interface and thus is less well suited as a library that allows the rapid development of new algorithms or integration into workflows.

CPPTraj~\cite{cpptraj-2013} offers multiple levels of parallelization (MPI and OpenMP) in a monolithic C++ implementation.
CCPTraj allows parallel reads between frames of the same trajectory but is especially geared towards processing an ensemble of many trajectories in parallel.

pyPcazip~\cite{pyPcazip} is a suite of software tools written in Python for compression and analysis of MD simulation data, in particular ensembles of trajectories. 
pyPcazip is MPI parallelised and is specific to PCA-based investigations of MD trajectories and supports a wide variety of trajectory file formats (based on the capabilities of the underlying mdtraj package~\cite{McGibbon:2015aa}).
pyPcazip can take one or many input MD trajectory files and convert them into a highly compressed, HDF5-based pcz format with insignificant loss of information.
However, the package does not support general purpose analysis.

\textit{In situ} analysis is an approach to execute analysis simultaneously with the running MD simulation so that I/O bottlenecks are  mitigated~\cite{Malakar-etal, Johnston:2017aa}.
Malakar \textit{et al.} studied the scalability challenges of time and space shared modes of analyzing large-scale MD simulations through a topology-aware mapping for simulation and analysis using the LAMMPS code \cite{Malakar-etal}.
Similarly, Taufer and colleagues \cite{Johnston:2017aa} presented their own framework for \textit{in situ} analysis, which is based on the fast on-the-fly calculation of metadata that characterizes protein substructures via maximum eigenvalues of distance matrices.
These metadata are used to index trajectory frames and enable targeted analysis of trajectory subsets.
Both studies provide important ideas and approaches towards moving towards online-analysis in conjunction with a running simulation but are limited in generality.

All of the above frameworks provide tools for parallel analysis of MD trajectories. 
These frameworks, however, tend to fall short in providing parallelism in the context of a general and flexible library for the analysis of MD trajectories.
Although straggler tasks are a common challenge arising in parallel analysis and are well-known in the data analysis community (see Section \ref{sec:stragglers}), there is, to our knowledge, little discussion about this problem in the biomolecular simulation community.
Our own experience with a MapReduce approach in \package{MDAnalysis}~\cite{Khoshlessan:2017ab} suggested that stragglers might be a somewhat under-appreciated problem.
Therefore, in the present work we want to better understand requirements for efficient parallel analysis of MD trajectories in \package{MDAnalysis}, but to also provide more general guidance that could benefit developments in other libraries inside and outside of the scope of analysis of MD simulations.




