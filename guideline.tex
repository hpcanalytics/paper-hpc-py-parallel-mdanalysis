% -*- mode: latex; mode: visual-line; fill-column: 9999; coding: utf-8 -*-

\section{Guidelines for Improving Parallel Trajectory Analysis Performance}
\label{sec:guidelines}

Although the performance measurements were performed with \package{MDAnalysis} and therefore capture some details of this library such as the specific timings for file reading, we believe that the broad picture is fairly general and applies to any Python-based approach that uses MPI for parallelizing trajectory access with a split-apply-combine approach.
Based on the lessons that we learned, we suggest the following guidelines to improve strong scaling performance:

Calculate the compute to I/O ratio ($\RcompIO$, Eq.~\ref{eq:Compute-IO}). As discussed in Section \ref{sec:bound}, for I/O bound problems the performance of the task will be affected by stragglers that delay job completion time.
\begin{description}
\item[\textbf{Heuristic 1}] For $\RcompIO \gg 1$, single-shared-file I/O can be used and one can expect reasonable scaling up to about 50 cores; for better scaling, one of the strategies under Heuristic 2 needs to be employed.
\item[\textbf{Heuristic 2}] For $\RcompIO \le 1$ the task is I/O bound and single-shared-file I/O should be avoided.
  One might want to consider the following steps:  
  \begin{description}
  \item[\textbf{Heuristic 2.1}] If there is access to the HDF5 format, use \textbf{MPI-based Parallel HDF5} (Section \ref{sec:HDF5}). This approach may scale well to hundreds of cores.
  \item[\textbf{Heuristic 2.2}] If the trajectory file is not in HDF5 format then one can consider \textbf{subfiling} and split the single trajectory file into as many trajectory segments as the number of processes. This approach may scale reasonably well to less than 200 cores.
  \end{description}
\end{description}

The better solution is the use of parallel I/O (\textbf{Heuristic 2.1}) as it makes best use of the parallel file system and scales well to hundreds of cores, regardless of $\RcompIO$. 
Splitting the trajectories will not scale as well as parallel I/O but it can be easily performed in parallel and trajectory conversion may be integrated into the beginning of standard workflows for MD simulations.  Alternatively, trajectories may already be kept in smaller chunks if they are already produced in batches; for instance, when running simulations with \package{Gromacs} \cite{Abraham:2015aa}, the \texttt{gmx mdrun -noappend} option produces individual trajectory segments instead of extending an existing trajectory file.
