\label{sec:guidelines}

Based on the performance statistics presented in the previous sections we formulate heuristics to improve the performance of parallel analysis of MD trajectories with \package{MDAnalysis} on HPC resources.
However, the proposed guidelines should also be more generally applicable to trajectory data analysis with other Python-based libraries.
To achieve near ideal scaling we suggest the following step by step guidelines:

\begin{description}
\item[\textbf{Heuristic 1}] Calculate compute to I/O ($\overline{\tcomp}/\overline{\tIO}$) ratio and compute to communication ($\overline{\tcomp}/\overline{\tcomm}$) ratio. The $\overline{\tcomp}/\overline{\tIO}$ ratio determines whether the task is compute bound ($\frac{\overline{\tcomp}}{\overline{\tIO}} \gg 1$) or IO bound ($\frac{\overline{\tcomp}}{\overline{\tIO}} \ll 1$).
The $\overline{\tcomp}/\overline{\tcomm}$ ratio determines whether the task is communication bound ($\frac{\overline{\tcomp}}{\overline{\tcomm}} \ll 1$) or compute bound ($\frac{\overline{\tcomp}}{\overline{\tIO}} \gg 1$).

As discussed in Section \ref{sec:bound}, for I/O bound problems the interference between MPI communication and I/O traffic can be problematic \cite{VMD2013, Kevin2018, Brown:2018ab} and the performance of the task will be affected by the straggler tasks which delay job completion time.
  
\item[\textbf{Heuristic 2}] For $\frac{\overline{\tcomp}}{\overline{\tIO}} \geqslant 1$, single-shared-file I/O can be used safely and will not affect performance depending on how large is compute with respect to I/O. Therefore, one needs to consider the following cases: 
  \begin{description}
  \item[\textbf{Heuristic 2.1}] If $\frac{\overline{\tcomp}}{\overline{\tcomm}} \gg 1$, the task is compute bound and will scale well as shown in Figure \ref{fig:tcomp_tIO_effect}. 
  \item[\textbf{Heuristic 2.2}] If $\frac{\overline{\tcomp}}{\overline{\tcomm}} \ll 1$, one might consider using Global Arrays to achieve near ideal scaling behavior (Section \ref{Global-Array}).  Application of Global Arrays replaces the message-passing interface with a distributed shared array where its blocks will be updated by the associated rank in the communication domain and collection of all data can be performed efficiently (Algorithm \ref{alg:GA}).
  \end{description}
\item[\textbf{Heuristic 3}] For $\frac{\overline{\tcomp}}{\overline{\tIO}} \leqslant 1$ the task is I/O bound and single-shared-file I/O should be avoided to remove unnecessary metadata operations. Therefore, one needs to take the following steps:  
  \begin{description}
    \item[\textbf{Heuristic 3.1}] If there is access to HDF5 format the recommended way will be to use MPI-based Parallel HDF5 (Section \ref{HDF5}). 
    \item[\textbf{Heuristic 3.2}] If the trajectory file is not in HDF5 format then one might prefer to split into as many trajectory segments as the number of processes.
      Splitting the trajectories can be easily performed in parallel as opposed to converting the XTC file to HDF5, which is computationally more expensive.
      MD trajectories are often re-analyzed and therefore incorporating trajectory conversion into the beginning of standard workflows for MD simulations could improve the performance of such workflows.
      Alternatively, it may be beneficial to keep the trajectories in smaller chunks, e.g., when running simulations on HPC resources using Gromacs \cite{Gromacs3, Gromacs1}, users can run their simulations with the ``-noappend" option so that output trajectories will be automatically stored in small chunks.
    \item[\textbf{Heuristic 3.3}] In case of $\frac{\overline{\tcomp}}{\overline{\tcomm}} \ll 1$, appropriate parallel implementation along with \emph{Global Arrays} should be used on the trajectory segments (Section \ref{splitting-traj}) to achieve near ideal scaling.
  \end{description}
\end{description}
