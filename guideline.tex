\label{sec:guidelines}

Although the performance measurements were performed with \package{MDAnalysis} and therefore capture some details of this  library such as the sepecific timings for file reading, we believe that the broad picture is fairly general and applies to any Python-based approach that uses MPI for parallelizing trajectory access.
Based on the lessons that we learned, we suggest the following guidelines to improve strong scaling performance:
\begin{description}
\item[\textbf{Heuristic 1}] Calculate compute to I/O ratio ($\RcompIO$, Eq.~\ref{eq:Compute-IO}) and compute to communication ratio ($\Rcompcomm$, Eq.~\ref{eq:Compute-comm}).
  $\RcompIO$ determines whether the task is compute bound ($\RcompIO \gg 1$) or IO bound ($\RcompIO \ll 1$).
  $\Rcompcomm$ determines whether the task is communication bound ($\frac{\overline{\tcomp}}{\overline{\tcomm}} \ll 1$) or compute bound ($\RcompIO \gg 1$).

  As discussed in Section \ref{sec:bound}, for I/O bound problems the interference between MPI communication and I/O traffic can be problematic \cite{VMD2013, Brown:2018ab} and the performance of the task will be affected by the straggler tasks which delay job completion time.
  
\item[\textbf{Heuristic 2}] For $\RcompIO \geqslant 1$, single-shared-file I/O can be used safely and will not affect performance depending on how large is compute with respect to I/O. Therefore, one needs to consider the following cases: 
  \begin{description}
  \item[\textbf{Heuristic 2.1}] If $\Rcompcomm \gg 1$, the task is compute bound and will scale well as shown in Figure \ref{fig:tcomp_tIO_effect}. 
  \item[\textbf{Heuristic 2.2}] If $\Rcompcomm \ll 1$, one might consider using \emph{Global Arrays} to improve scaling by utilizing efficient distribution of data via the shared arrays (section \ref{Global-Array}).
  \end{description}
\item[\textbf{Heuristic 3}] For $\RcompIO \leqslant 1$ the task is I/O bound and single-shared-file I/O should be avoided to remove unnecessary metadata operations.
  One might want to consider the following steps:  
  \begin{description}
    \item[\textbf{Heuristic 3.1}] If there is access to HDF5 format, use MPI-based Parallel HDF5 (Section \ref{HDF5}). 
    \item[\textbf{Heuristic 3.2}] If the trajectory file is not in HDF5 format then one might prefer to apply subfiling and split the single trajectory file into as many trajectory segments as the number of processes.
      Splitting the trajectories can be easily performed in parallel as opposed to converting the XTC file to HDF5, which is computationally more expensive.
      MD trajectories are often re-analyzed and therefore incorporating trajectory conversion into the beginning of standard workflows for MD simulations could improve the performance of such workflows.
      Alternatively, it may be beneficial to keep the trajectories in smaller chunks, e.g., when running simulations on HPC resources using Gromacs \cite{Abraham:2015aa}, users can run their simulations with the ``-noappend" option so that output trajectories will be automatically stored in small chunks.
    \item[\textbf{Heuristic 3.3}] In case of $\Rcompcomm \ll 1$, appropriate parallel implementation along with \emph{Global Arrays} should be used on the trajectory segments (Section \ref{splitting-traj}) to achieve near ideal scaling.
  \end{description}
\end{description}
