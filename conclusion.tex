% -*- mode: latex; mode: visual-line; fill-column: 9999; coding: utf-8 -*-

\section{Conclusions}
\label{sec:conclusions}

We analyzed the strong scaling performance of a typical task when analyzing MD trajectories, the calculation of the time series of the RMSD of a protein, with the widely used Python-based \package{MDAnalysis} library.
All benchmarks were performed in five replicates on three different XSEDE supercomputers to demonstrate that our results were independent from the specifics of the hardware and local environment.

The RMSD task was parallelized with MPI following the \emph{split-apply-combine} approach by having each MPI process analyze a contiguous segment of the trajectory.
This approach did not scale beyond a single node because straggler MPI processes exhibited large upward variations in runtime.
Stragglers were primarily caused by either increased MPI communication costs or increased time to open the single shared trajectory file whereas both the computation and the ingestion of data exhibited close to ideal strong scaling behavior.
Stragglers were less prevalent for compute-bound workloads (i.e., $\RcompIO \gg 1$), suggesting that file read I/O was responsible for poor MPI communication.
In particular, artificially removing all I/O substantially improved performance of the communication step and thus brought overall performance close to ideal (i.e., linear increase in speed-up with processor count with slope one), despite the fact that the amount of data to be communicated did not depend on I/O.
Our results suggested that stragglers might be due to the competition between MPI messages and the Lustre file system on the shared InfiniBand interconnect, which would be consistent with other similar observations \cite{VMD2013} and theoretical predictions by \citet{Brown:2018ab}, but further work would be needed to validate this specific hypothesis.
One possible interpretation of our results was that for a sufficiently large per-frame compute workload, read I/O interfered much less with communication than for an I/O bound task that almost continuously accesses the file system.
This interpretation suggested that the poor scaling performance was the result of inefficient use of the Lustre file system and that we needed to improve read I/O to reduce interference.

We investigated subfiling (splitting of the trajectories into separate files, one for each MPI rank) and MPI-based parallel I/O.
Subfiling improved scaling up to about 150 cores.
However, subfiling, at least in the form described here, is not an ideal solution because creating and accessing many small files on a parallel file system such as Lustre can negatively impact the overall performance of the file system.
Furthermore, managing a large number of files can become cumbersome and inflexible, given that the number of files determines the number of processes.
When we used MPI-based parallel I/O through HDF5 together with MPI for communications we achieved nearly ideal performance up to 384 cores (16 nodes on \emph{SDSC Comet}) and speed-ups of two orders of magnitude compared to the serial execution.
The latter approach appears to be a promising way forward as it directly builds on very widely used technology (MPI-IO and HDF5) and echoes the experience of the wider HPC community that parallel file I/O is necessary for efficient data handling.

The biomolecular simulation community suffers from a large number of trajectory file formats with very few being based on HDF5, with the exception of the H5MD format \cite{Buyl:2014aa} and the MDTraj HDF5 format \cite{McGibbon:2015aa}.
Our work suggests that HDF5-based formats should be seriously considered for MD simulations if users want to make efficient use of their HPC systems for analysis. 
Alternatively, enabling MPI-IO for trajectory readers in libraries such as \package{MDAnalysis} might also provide a path forward to better read performance.

We summarized our findings in a number of guidelines for improving the scaling of parallel analysis of MD trajectory data.
We showed that it is feasible to run an I/O bound analysis task on HPC resources with a Lustre parallel file system and achieve good scaling behavior up to 384 CPU cores with an almost 300-fold speed-up compared to serial execution.

\paragraph{Future Directions}

Future work might look into testing different MPI implementations, especially in combination with parallel HDF5.
Choosing the best performing MPI implementation for a specific system and optimizing the parallel file system parameters might also lead to further improvements.
Although our results showed qualitatively similar behavior on three different HPC resources, unexplained differences in performances remained.
Deeper insights into the system-level network traffic and parallel file system access would be necessary to approach performance tuning for different HPC systems in a rational manner.

Our HDF5 results are encouraging but lack a convenient and widely available implementation.
Therefore, a HDF5-based trajectory reader needs to be implemented in MDAnalysis for an existing HDF5 trajectory format.
The algorithm for the analysis task could be optimized by reducing file access to the shared system topology file (and any other data common to all ranks, such as the reference coordinates in the RMSD analysis) by using \texttt{MPI\_Scatter} and \texttt{MPI\_Gather} to efficiently communicate the static data.
In this case, only rank 0 would read these data from the file system and then scatter them to all other ranks.
Each rank would then build their own MDAnalysis \texttt{Universe} from those data (either by gathering a serialized \texttt{Universe} data structure or by using Python \texttt{StringIO} to read the scattered text buffer containing the topology file) and their own parallel file access to an HDF5 trajectory (with the \texttt{Universe.load\_new()} method to attach a trajectory).

In summary, the encouraging finding of this work is that by using parallel file reading (here tested with HDF5), the simple split-apply-combine single trajectory parallelization approach can work on current HPC systems up to a few hundred cores, even for I/O-bound tasks.
The major advantage of the approach is its simplicity as users can directly use their serial code and apply it to blocks of a trajectory, without having to rewrite their algorithms or having to consider hybrid parallelization schemes. 
Although we focused on the \package{MDAnalysis} library, similar strategies are likely to be more generally applicable and useful to the wider biomolecular simulation community.




